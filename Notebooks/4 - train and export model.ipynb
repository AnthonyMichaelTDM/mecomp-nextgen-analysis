{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e689e09",
   "metadata": {},
   "source": [
    "# Train and export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e33151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Batch size: 12\n",
      "Max audio duration: 15.0s\n",
      "Frame size: 2048\n",
      "TensorBoard logs: /home/anthony/Sync/mecomp-nextgen-analysis/Notebooks/../models/runs\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuration\n",
    "SAMPLE_RATE = 22050  # Hz\n",
    "EMBEDDING_DIM = 32   # Output embedding size\n",
    "HIDDEN_DIM = 256     # GRU hidden dimension\n",
    "NUM_LAYERS = 2       # Number of GRU layers\n",
    "BATCH_SIZE = 12      # Small batch size to fit in GPU memory\n",
    "FT_BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-3\n",
    "FT_LEARNING_RATE = 1e-5 # Learning rate during fine-tuning\n",
    "NUM_EPOCHS = 50\n",
    "FT_NUM_EPOCHS = 20\n",
    "MAX_AUDIO_DURATION = 15.0  # Max duration in seconds\n",
    "FT_MAX_AUDIO_DURATION = 29.0  # Max duration in seconds\n",
    "FRAME_SIZE = 2048    # Frame size for CNN frontend\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "FMA_SMALL_DIR = DATA_DIR / \"fma_small\"\n",
    "HUMAN_TRIPLETS_PATH = DATA_DIR / \"human_triplets.csv\"\n",
    "SYNTHETIC_TRIPLETS_PATH = DATA_DIR / \"synthetic_triplets.csv\"\n",
    "MODEL_SAVE_PATH = Path(\"../models\")\n",
    "MODEL_SAVE_PATH.mkdir(exist_ok=True)\n",
    "TENSORBOARD_LOG_DIR = Path(\"../models/runs\")\n",
    "TENSORBOARD_LOG_DIR.mkdir(exist_ok=True)\n",
    "AUDIO_CACHE_PICKLE_PATH = DATA_DIR / \"audio_cache.pkl\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Max audio duration: {MAX_AUDIO_DURATION}s\")\n",
    "print(f\"Frame size: {FRAME_SIZE}\")\n",
    "print(f\"TensorBoard logs: {TENSORBOARD_LOG_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f2c2c",
   "metadata": {},
   "source": [
    "## Load and process data\n",
    "\n",
    "Load the human and synthetic triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecac1d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human triplets after filtering: 50\n",
      "Human triplets: 50\n",
      "Synthetic triplets: 8000\n",
      "\n",
      "Human triplets sample:\n",
      "   anchor  positive  negative\n",
      "0  107389     25033    142433\n",
      "1  141164    123976    116883\n",
      "2   73675      5940     53586\n",
      "3  108808     12189     38450\n",
      "4  150078     25797    123834\n",
      "\n",
      "Synthetic triplets sample:\n",
      "   anchor  positive  negative\n",
      "0  145742     91306    108867\n",
      "1  123979     89846     90590\n",
      "2  123969     75930    152570\n",
      "3  123442    137900     71276\n",
      "4  141562     13767     59727\n"
     ]
    }
   ],
   "source": [
    "# Load triplet data\n",
    "human_triplets = pd.read_csv(HUMAN_TRIPLETS_PATH)\n",
    "synthetic_triplets = pd.read_csv(SYNTHETIC_TRIPLETS_PATH, skiprows=1)  # Skip the header row with column types\n",
    "\n",
    "# Filter human triplets to only keep rows where anchor and positive are set\n",
    "human_triplets = human_triplets.dropna(subset=['anchor', 'positive'])\n",
    "print(f\"Human triplets after filtering: {len(human_triplets)}\")\n",
    "\n",
    "# Set field types\n",
    "COLUMNS = [\n",
    "    \"anchor\",\n",
    "    \"positive\"\n",
    "]\n",
    "for column in COLUMNS:\n",
    "    human_triplets[column] = human_triplets[column].astype(int)\n",
    "\n",
    "# The synthetic triplets have duplicate column names, let's clean them up\n",
    "# Keep only the first 3 columns (anchor, positive, negative track_ids)\n",
    "synthetic_triplets = synthetic_triplets.iloc[:, :3]\n",
    "synthetic_triplets.columns = ['anchor', 'positive', 'negative']\n",
    "# synthetic_triplets = synthetic_triplets.sample(1000) # if we can't overfit on a subset, we aren't expressive enough\n",
    "\n",
    "# Human triplets format: song1, song2, song3, anchor, positive\n",
    "# We need to convert to anchor, positive, negative format\n",
    "def convert_human_triplet(row):\n",
    "    \"\"\"Convert human triplet format to anchor/positive/negative format.\"\"\"\n",
    "    songs = [row['song1'], row['song2'], row['song3']]\n",
    "    anchor_idx = row['anchor'] - 1  # Convert 1-indexed to 0-indexed\n",
    "    positive_idx = row['positive'] - 1\n",
    "    negative_idx = 3 - anchor_idx - positive_idx  # The remaining index\n",
    "    return pd.Series({\n",
    "        'anchor': songs[anchor_idx],\n",
    "        'positive': songs[positive_idx],\n",
    "        'negative': songs[negative_idx]\n",
    "    }, dtype=int)\n",
    "\n",
    "human_triplets_converted = human_triplets.apply(convert_human_triplet, axis=1)\n",
    "\n",
    "print(f\"Human triplets: {len(human_triplets_converted)}\")\n",
    "print(f\"Synthetic triplets: {len(synthetic_triplets)}\")\n",
    "print(f\"\\nHuman triplets sample:\")\n",
    "print(human_triplets_converted.head())\n",
    "print(f\"\\nSynthetic triplets sample:\")\n",
    "print(synthetic_triplets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef65fbe",
   "metadata": {},
   "source": [
    "### split synthetic triples into train test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a15a185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Round Training:\n",
      "Training triplets: 5600 (synthetic only)\n",
      "Validation triplets: 792 (synthetic only)\n",
      "Test triplets: 1608 (synthetic only)\n",
      "Finetuning:\n",
      "Training triplets: 25 (human only)\n",
      "Validation triplets: 13 (human only)\n",
      "Test triplets: 12 (human only)\n"
     ]
    }
   ],
   "source": [
    "# Split the human and synthetic triplets into train/validation/test sets\n",
    "# For human (finetuning):\n",
    "# - 70% train\n",
    "# - 20% test\n",
    "# - 10% validation\n",
    "# for synthetic (training):\n",
    "# - 50% train\n",
    "# - 25% test\n",
    "# - 25% validation\n",
    "\n",
    "\n",
    "train_triplets, synthetic_temp = train_test_split(\n",
    "    synthetic_triplets, test_size=0.3, random_state=42\n",
    ")\n",
    "test_triplets, val_triplets = train_test_split(\n",
    "    synthetic_temp, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "ft_train_triplets, human_temp = train_test_split(\n",
    "    human_triplets_converted, test_size=0.5, random_state=42\n",
    ")\n",
    "ft_test_triplets, ft_val_triplets = train_test_split(\n",
    "    human_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Combine synthetic triplets with human data for ... dataset\n",
    "# test_triplets = pd.concat([human_test, synthetic_test], ignore_index=True)\n",
    "\n",
    "# (human: {len(human_test)}, synthetic: {len(synthetic_test)})\n",
    "print(\"First Round Training:\")\n",
    "print(f\"Training triplets: {len(train_triplets)} (synthetic only)\")\n",
    "print(f\"Validation triplets: {len(val_triplets)} (synthetic only)\")\n",
    "print(\n",
    "    f\"Test triplets: {len(test_triplets)} (synthetic only)\"\n",
    ")\n",
    "print(\"Finetuning:\")\n",
    "\n",
    "print(f\"Training triplets: {len(ft_train_triplets)} (human only)\")\n",
    "print(f\"Validation triplets: {len(ft_val_triplets)} (human only)\")\n",
    "print(f\"Test triplets: {len(ft_test_triplets)} (human only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7233c7",
   "metadata": {},
   "source": [
    "### create a dataloader with the raw audio for the songs for each triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd313e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_audio_path(track_id: int) -> Path:\n",
    "    \"\"\"Get the path to an audio file given its track ID.\"\"\"\n",
    "    # Track IDs are stored in folders based on first 3 digits\n",
    "    # e.g., track 123456 would be in fma_small/123/123456.mp3\n",
    "    tid_str = f\"{track_id:06d}\"\n",
    "    folder = tid_str[:3]\n",
    "    return FMA_SMALL_DIR / folder / f\"{tid_str}.mp3\"\n",
    "\n",
    "\n",
    "def load_audio_ffmpeg(path: Path, sample_rate: int = SAMPLE_RATE) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load audio file using ffmpeg and resample to target sample rate.\n",
    "    Returns mono audio as numpy array of float32 samples normalized to [-1, 1].\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    \n",
    "    cmd = [\n",
    "        'ffmpeg',\n",
    "        '-i', str(path),\n",
    "        '-f', 'f32le',        # 32-bit float little-endian PCM\n",
    "        '-acodec', 'pcm_f32le',\n",
    "        '-ac', '1',           # Mono\n",
    "        '-ar', str(sample_rate),  # Resample\n",
    "        '-v', 'quiet',        # Suppress output\n",
    "        '-'                   # Output to stdout\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, check=True)\n",
    "        # Use .copy() to make the array writable (frombuffer returns read-only view)\n",
    "        audio = np.frombuffer(result.stdout, dtype=np.float32).copy()\n",
    "        return audio\n",
    "    except subprocess.CalledProcessError:\n",
    "        return None\n",
    "\n",
    "AUDIO_CACHE: dict[int, torch.Tensor] = {}\n",
    "if AUDIO_CACHE_PICKLE_PATH.exists():\n",
    "    AUDIO_CACHE = torch.load(AUDIO_CACHE_PICKLE_PATH)\n",
    "\n",
    "class TripletAudioDataset(Dataset):\n",
    "    \"\"\"Dataset that loads triplets of audio files.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        triplets_df: pd.DataFrame, \n",
    "        fma_dir: Path,\n",
    "        sample_rate: int | None = None,\n",
    "        max_duration: float = 30.0,  # Max duration in seconds\n",
    "        cache_audio: bool = False\n",
    "    ):\n",
    "        global AUDIO_CACHE\n",
    "        self.triplets: pd.DataFrame = triplets_df.reset_index(drop=True)\n",
    "        self.fma_dir: Path = fma_dir\n",
    "        self.sample_rate: int = sample_rate or SAMPLE_RATE\n",
    "        self.max_duration: float = max_duration\n",
    "        self.max_samples = int(max_duration * self.sample_rate)\n",
    "        self.cache_audio: bool = cache_audio\n",
    "\n",
    "        # Filter out triplets where any audio file is missing\n",
    "        valid_indices = []\n",
    "        updated_audio_cache = False\n",
    "        for idx in tqdm(range(len(self.triplets)), desc=\"Validating audio files\"):\n",
    "            row = self.triplets.iloc[idx]\n",
    "            anchor_path = get_audio_path(int(row['anchor']))\n",
    "            positive_path = get_audio_path(int(row['positive']))\n",
    "            negative_path = get_audio_path(int(row['negative']))\n",
    "\n",
    "            if anchor_path.exists() and positive_path.exists() and negative_path.exists():\n",
    "                valid_indices.append(idx)\n",
    "\n",
    "                if self.cache_audio:\n",
    "                    if int(row['anchor']) not in AUDIO_CACHE:\n",
    "                        AUDIO_CACHE[int(row['anchor'])] = self._load_audio(int(row['anchor']))\n",
    "                        updated_audio_cache = True\n",
    "                    if int(row['positive']) not in AUDIO_CACHE:\n",
    "                        AUDIO_CACHE[int(row['positive'])] = self._load_audio(int(row['positive']))\n",
    "                        updated_audio_cache = True\n",
    "                    if int(row['negative']) not in AUDIO_CACHE:\n",
    "                        AUDIO_CACHE[int(row['negative'])] = self._load_audio(int(row['negative']))\n",
    "                        updated_audio_cache = True\n",
    "                    if idx % 1000 == 0 and updated_audio_cache:\n",
    "                        torch.save(AUDIO_CACHE, AUDIO_CACHE_PICKLE_PATH)\n",
    "\n",
    "        if self.cache_audio and updated_audio_cache:\n",
    "            torch.save(AUDIO_CACHE, AUDIO_CACHE_PICKLE_PATH)\n",
    "\n",
    "        self.triplets = self.triplets.iloc[valid_indices].reset_index(drop=True)\n",
    "        print(f\"Valid triplets: {len(self.triplets)} (removed {len(triplets_df) - len(self.triplets)} invalid)\")\n",
    "\n",
    "    def _load_audio(self, track_id: int) -> torch.Tensor:\n",
    "        \"\"\"Load and preprocess audio for a track.\"\"\"\n",
    "        global AUDIO_CACHE\n",
    "        if self.cache_audio and track_id in AUDIO_CACHE:\n",
    "            audio_tensor = AUDIO_CACHE[track_id]\n",
    "            # Pad or truncate to max_samples\n",
    "            if len(audio_tensor) < self.max_samples:\n",
    "                audio_tensor = torch.nn.functional.pad(\n",
    "                    audio_tensor, (0, self.max_samples - len(audio_tensor))\n",
    "                )\n",
    "            else:\n",
    "                audio_tensor = audio_tensor[: self.max_samples]\n",
    "            return audio_tensor\n",
    "\n",
    "        path = get_audio_path(track_id)\n",
    "        audio = load_audio_ffmpeg(path, self.sample_rate)\n",
    "\n",
    "        if audio is None:\n",
    "            # Return silence if loading fails\n",
    "            audio = np.zeros(self.max_samples, dtype=np.float32)\n",
    "\n",
    "        audio_tensor = torch.from_numpy(audio).float()\n",
    "        if self.cache_audio:\n",
    "            AUDIO_CACHE[track_id] = audio_tensor\n",
    "\n",
    "        # Pad or truncate to max_samples\n",
    "        if len(audio_tensor) < self.max_samples:\n",
    "            audio_tensor = torch.nn.functional.pad(audio_tensor, (0, self.max_samples - len(audio_tensor)))\n",
    "        else:\n",
    "            audio_tensor = audio_tensor[: self.max_samples]\n",
    "\n",
    "        return audio_tensor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        row = self.triplets.iloc[idx]\n",
    "\n",
    "        anchor = self._load_audio(int(row['anchor']))\n",
    "        positive = self._load_audio(int(row['positive']))\n",
    "        negative = self._load_audio(int(row['negative']))\n",
    "\n",
    "        return anchor, positive, negative\n",
    "\n",
    "\n",
    "def collate_triplets(batch: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]):\n",
    "    \"\"\"Collate function for triplet batches.\"\"\"\n",
    "    anchors = torch.stack([item[0] for item in batch])\n",
    "    positives = torch.stack([item[1] for item in batch])\n",
    "    negatives = torch.stack([item[2] for item in batch])\n",
    "    return anchors, positives, negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a90bdc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8df612de4e64ca6bf703b047f7405fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating audio files:   0%|          | 0/5600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid triplets: 5600 (removed 0 invalid)\n",
      "\n",
      "Creating validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067411aa551c45368ea4297df0c91273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating audio files:   0%|          | 0/792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid triplets: 792 (removed 0 invalid)\n",
      "\n",
      "Creating test dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fd1262e2d945adbe24aadcbd59250b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating audio files:   0%|          | 0/1608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid triplets: 1608 (removed 0 invalid)\n",
      "\n",
      "Train batches: 467\n",
      "Val batches: 66\n",
      "Test batches: 134\n"
     ]
    }
   ],
   "source": [
    "# Create datasets with reduced duration\n",
    "print(\"Creating training dataset...\")\n",
    "train_dataset = TripletAudioDataset(\n",
    "    train_triplets,\n",
    "    FMA_SMALL_DIR,\n",
    "    max_duration=MAX_AUDIO_DURATION,\n",
    "    cache_audio=True,\n",
    ")\n",
    "\n",
    "print(\"\\nCreating validation dataset...\")\n",
    "val_dataset = TripletAudioDataset(\n",
    "    val_triplets,\n",
    "    FMA_SMALL_DIR,\n",
    "    max_duration=MAX_AUDIO_DURATION,\n",
    "    cache_audio=True,\n",
    ")\n",
    "\n",
    "print(\"\\nCreating test dataset...\")\n",
    "test_dataset = TripletAudioDataset(\n",
    "    test_triplets,\n",
    "    FMA_SMALL_DIR,\n",
    "    max_duration=MAX_AUDIO_DURATION,\n",
    "    cache_audio=True,\n",
    ")\n",
    "\n",
    "# Create dataloaders (num_workers=0 to avoid multiprocessing issues in notebooks)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_triplets,\n",
    "    num_workers=0,\n",
    "    \n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_triplets,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_triplets,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e21df",
   "metadata": {},
   "source": [
    "## Define the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da69210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioEmbeddingGRU(\n",
      "  (cnn_frontend): Sequential(\n",
      "    (0): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv1d(64, 128, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): AdaptiveAvgPool1d(output_size=1)\n",
      "  )\n",
      "  (gru): GRU(128, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (projection): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=32, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 807,200\n",
      "Trainable parameters: 807,200\n"
     ]
    }
   ],
   "source": [
    "class AudioEmbeddingGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU-based model for generating audio embeddings from raw audio frames.\n",
    "    \n",
    "    The model processes raw audio samples through:\n",
    "    1. A 1D CNN frontend to extract local features from the raw waveform\n",
    "    2. A GRU to capture temporal dependencies across the audio\n",
    "    3. A projection layer to produce the final embedding\n",
    "    \n",
    "    Args:\n",
    "        embedding_dim: Dimension of the output embedding (default: 128)\n",
    "        hidden_dim: Dimension of GRU hidden state (default: 256)\n",
    "        num_layers: Number of GRU layers (default: 2)\n",
    "        sample_rate: Expected sample rate of input audio (default: 22050)\n",
    "        frame_size: Size of each frame for the CNN frontend (default: 1024)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int = 128,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2,\n",
    "        sample_rate: int = 22050,\n",
    "        frame_size: int = 1024,\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.sample_rate = sample_rate\n",
    "        self.frame_size = frame_size\n",
    "        \n",
    "        # CNN frontend to extract features from raw audio frames\n",
    "        # Input: (batch, 1, frame_size) -> Output: (batch, cnn_out_dim)\n",
    "        self.cnn_frontend = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # # Third conv block\n",
    "            # nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.BatchNorm1d(256),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Global average pooling\n",
    "        )\n",
    "        \n",
    "        # Calculate CNN output dimension\n",
    "        self.cnn_out_dim = 128\n",
    "        \n",
    "        # GRU for temporal modeling\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.cnn_out_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Projection head to embedding space\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # L2 normalization for embeddings\n",
    "        self.normalize = True\n",
    "    \n",
    "    def _extract_frames(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract non-overlapping frames from raw audio.\n",
    "        Each frame is independent - no overlap between frames.\n",
    "        \n",
    "        Args:\n",
    "            audio: Raw audio tensor of shape (batch, num_samples)\n",
    "            \n",
    "        Returns:\n",
    "            Frames tensor of shape (batch, num_frames, frame_size)\n",
    "        \"\"\"\n",
    "        batch_size, num_samples = audio.shape\n",
    "        \n",
    "        # Calculate number of complete frames (non-overlapping)\n",
    "        num_frames = num_samples // self.frame_size\n",
    "        \n",
    "        # Truncate to complete frames only\n",
    "        usable_samples = num_frames * self.frame_size\n",
    "        audio_truncated = audio[:, :usable_samples]\n",
    "        \n",
    "        # Reshape to (batch, num_frames, frame_size)\n",
    "        frames = audio_truncated.view(batch_size, num_frames, self.frame_size)\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    def forward(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate embedding from raw audio.\n",
    "        \n",
    "        Args:\n",
    "            audio: Raw audio tensor of shape (batch, num_samples)\n",
    "            \n",
    "        Returns:\n",
    "            Embedding tensor of shape (batch, embedding_dim)\n",
    "        \"\"\"\n",
    "        batch_size = audio.shape[0]\n",
    "        \n",
    "        # Extract frames from audio (non-overlapping)\n",
    "        frames = self._extract_frames(audio)  # (batch, num_frames, frame_size)\n",
    "        num_frames = frames.shape[1]\n",
    "        \n",
    "        # Reshape for CNN: (batch * num_frames, 1, frame_size)\n",
    "        frames_flat = frames.reshape(-1, 1, self.frame_size)\n",
    "        \n",
    "        # Apply CNN frontend\n",
    "        cnn_out = self.cnn_frontend(frames_flat)  # (batch * num_frames, cnn_out_dim, 1)\n",
    "        cnn_out = cnn_out.squeeze(-1)  # (batch * num_frames, cnn_out_dim)\n",
    "        \n",
    "        # Reshape back to sequence: (batch, num_frames, cnn_out_dim)\n",
    "        cnn_out = cnn_out.view(batch_size, num_frames, -1)\n",
    "        \n",
    "        # Apply GRU\n",
    "        gru_out, hidden = self.gru(cnn_out)  # gru_out: (batch, num_frames, hidden_dim)\n",
    "        \n",
    "        # Use final hidden state for embedding\n",
    "        final_hidden = hidden[-1]  # (batch, hidden_dim)\n",
    "        \n",
    "        # Project to embedding space\n",
    "        embedding = self.projection(final_hidden)  # (batch, embedding_dim)\n",
    "        \n",
    "        # L2 normalize embeddings\n",
    "        if self.normalize:\n",
    "            embedding = F.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def get_embedding_dim(self) -> int:\n",
    "        \"\"\"Return the embedding dimension.\"\"\"\n",
    "        return self.embedding_dim\n",
    "\n",
    "\n",
    "# Create model instance with configured frame size\n",
    "model = AudioEmbeddingGRU(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    frame_size=FRAME_SIZE,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5afd3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioEmbeddingTiny(\n",
      "  (cnn_frontend): Sequential(\n",
      "    (0): DepthwiseBlock(\n",
      "      (net): Sequential(\n",
      "        (0): Conv1d(1, 1, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Conv1d(1, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "    (1): DepthwiseBlock(\n",
      "      (net): Sequential(\n",
      "        (0): Conv1d(32, 32, kernel_size=(5,), stride=(2,), padding=(2,), groups=32, bias=False)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "    (2): AdaptiveAvgPool1d(output_size=1)\n",
      "  )\n",
      "  (gru): GRU(64, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (attn): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (projection): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=32, bias=True)\n",
      "  )\n",
      ")\n",
      "Params: 784,616\n"
     ]
    }
   ],
   "source": [
    "class DepthwiseBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=5, s=2, p=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_ch,\n",
    "                in_ch,\n",
    "                kernel_size=k,\n",
    "                stride=s,\n",
    "                padding=p,\n",
    "                groups=in_ch,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm1d(in_ch),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm1d(out_ch),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class AudioEmbeddingTiny(nn.Module):\n",
    "    \"\"\"\n",
    "    Raw-audio → depthwise CNN → GRU → attention pooling → projection + L2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int = 32,\n",
    "        cnn_channels: tuple[int, ...] = (32, 64, 96),\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 1,\n",
    "        sample_rate: int = 22050,\n",
    "        frame_size: int = 1024,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.frame_size = frame_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        blocks = []\n",
    "        in_ch = 1\n",
    "        for ch in cnn_channels:\n",
    "            blocks.append(DepthwiseBlock(in_ch, ch, k=5, s=2, p=2))\n",
    "            in_ch = ch\n",
    "        blocks.append(nn.AdaptiveAvgPool1d(1))  # pool per frame\n",
    "        self.cnn_frontend = nn.Sequential(*blocks)\n",
    "        self.cnn_out_dim = in_ch\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.cnn_out_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        # Attention pooling over time\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def _extract_frames(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        # audio: (B, T); returns (B, num_frames, frame_size)\n",
    "        b, t = audio.shape\n",
    "        num_frames = t // self.frame_size\n",
    "        usable = num_frames * self.frame_size\n",
    "        audio = audio[:, :usable]\n",
    "        frames = audio.view(b, num_frames, self.frame_size)\n",
    "        return frames\n",
    "\n",
    "    def forward(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        b = audio.shape[0]\n",
    "        frames = self._extract_frames(audio)  # (B, F, L)\n",
    "        frames_flat = frames.reshape(-1, 1, self.frame_size)  # (B*F, 1, L)\n",
    "        cnn_out = self.cnn_frontend(frames_flat).squeeze(-1)  # (B*F, C)\n",
    "        seq = cnn_out.view(b, -1, self.cnn_out_dim)  # (B, F, C)\n",
    "\n",
    "        gru_out, _ = self.gru(seq)  # (B, F, H)\n",
    "\n",
    "        # Attention pooling\n",
    "        attn_scores = self.attn(gru_out).squeeze(-1)  # (B, F)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(-1)\n",
    "        pooled = (gru_out * attn_weights).sum(dim=1)  # (B, H)\n",
    "\n",
    "        emb = self.projection(pooled)  # (B, D)\n",
    "        emb = F.normalize(emb, p=2, dim=1)\n",
    "        return emb\n",
    "\n",
    "    def get_embedding_dim(self) -> int:\n",
    "        return self.embedding_dim\n",
    "\n",
    "\n",
    "# Example instantiation mirroring your current config\n",
    "model = AudioEmbeddingTiny(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    cnn_channels=(32, 64),\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    frame_size=FRAME_SIZE,  # match your FRAME_SIZE\n",
    "    dropout=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\n",
    "    f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc300bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With 15.0s audio @ 22050Hz:\n",
      "  Samples per audio: 330,750\n",
      "  Frames per audio: 161\n",
      "  Frames per batch (×3 for triplet): 5,796\n",
      "\n",
      "Test input shape: torch.Size([2, 330750])\n",
      "Test output shape: torch.Size([2, 32])\n"
     ]
    }
   ],
   "source": [
    "# Calculate memory usage estimate\n",
    "num_samples = int(SAMPLE_RATE * MAX_AUDIO_DURATION)\n",
    "num_frames = num_samples // FRAME_SIZE\n",
    "print(f\"\\nWith {MAX_AUDIO_DURATION}s audio @ {SAMPLE_RATE}Hz:\")\n",
    "print(f\"  Samples per audio: {num_samples:,}\")\n",
    "print(f\"  Frames per audio: {num_frames}\")\n",
    "print(f\"  Frames per batch (×3 for triplet): {BATCH_SIZE * 3 * num_frames:,}\")\n",
    "\n",
    "# Quick test with dummy input\n",
    "dummy_input = torch.randn(2, int(SAMPLE_RATE * MAX_AUDIO_DURATION)).to(DEVICE)\n",
    "dummy_output = model(dummy_input)\n",
    "print(f\"\\nTest input shape: {dummy_input.shape}\")\n",
    "print(f\"Test output shape: {dummy_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08c248",
   "metadata": {},
   "source": [
    "## Train and Test loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ee93b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletMarginLossWithAccuracy(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet margin loss with accuracy tracking.\n",
    "    \n",
    "    For triplet (anchor, positive, negative), the loss is:\n",
    "    max(0, margin + d(anchor, positive) - d(anchor, negative))\n",
    "    \n",
    "    Accuracy is measured as the percentage of triplets where:\n",
    "    d(anchor, positive) < d(anchor, negative)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, margin: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        anchor: torch.Tensor, \n",
    "        positive: torch.Tensor, \n",
    "        negative: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, float, float, float]:\n",
    "        \"\"\"\n",
    "        Compute triplet loss and accuracy.\n",
    "        \n",
    "        Args:\n",
    "            anchor: Anchor embeddings (batch, embedding_dim)\n",
    "            positive: Positive embeddings (batch, embedding_dim)\n",
    "            negative: Negative embeddings (batch, embedding_dim)\n",
    "            \n",
    "        Returns:\n",
    "            loss: Scalar loss tensor\n",
    "            accuracy: Float accuracy (0-1)\n",
    "            avg_dist_pos: Average distance to positive\n",
    "            avg_dist_neg: Average distance to negative\n",
    "        \"\"\"\n",
    "        # Compute L2 distances\n",
    "        dist_pos = F.pairwise_distance(anchor, positive, p=2)\n",
    "        dist_neg = F.pairwise_distance(anchor, negative, p=2)\n",
    "        \n",
    "        # Triplet margin loss\n",
    "        losses = F.relu(self.margin + dist_pos - dist_neg)\n",
    "        loss = losses.mean()\n",
    "        \n",
    "        # Accuracy: positive should be closer than negative\n",
    "        correct = (dist_pos < dist_neg).float()\n",
    "        accuracy = correct.mean().item()\n",
    "        \n",
    "        # Average distances for logging\n",
    "        avg_dist_pos = dist_pos.mean().item()\n",
    "        avg_dist_neg = dist_neg.mean().item()\n",
    "        \n",
    "        return loss, accuracy, avg_dist_pos, avg_dist_neg\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    writer: Optional[SummaryWriter] = None,\n",
    "    global_step: int = 0\n",
    ") -> Tuple[float, float, float, float, int]:\n",
    "    \"\"\"Train for one epoch with TensorBoard logging.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_dist_pos = 0.0\n",
    "    total_dist_neg = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch} [Train]\")\n",
    "    for anchors, positives, negatives in pbar:\n",
    "        anchors = anchors.to(device)\n",
    "        positives = positives.to(device)\n",
    "        negatives = negatives.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        anchor_emb = model(anchors)\n",
    "        positive_emb = model(positives)\n",
    "        negative_emb = model(negatives)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, accuracy, dist_pos, dist_neg = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy\n",
    "        total_dist_pos += dist_pos\n",
    "        total_dist_neg += dist_neg\n",
    "        num_batches += 1\n",
    "        global_step += 1\n",
    "        \n",
    "        # Log batch metrics to TensorBoard\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('Train/BatchLoss', loss.item(), global_step)\n",
    "            writer.add_scalar('Train/BatchAccuracy', accuracy, global_step)\n",
    "            writer.add_scalar('Train/GradNorm', grad_norm.item(), global_step)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{accuracy:.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "    avg_dist_pos = total_dist_pos / num_batches\n",
    "    avg_dist_neg = total_dist_neg / num_batches\n",
    "    \n",
    "    return avg_loss, avg_accuracy, avg_dist_pos, avg_dist_neg, global_step\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    desc: str = \"Eval\"\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"Evaluate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_dist_pos = 0.0\n",
    "    total_dist_neg = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for anchors, positives, negatives in tqdm(dataloader, desc=desc):\n",
    "        anchors = anchors.to(device)\n",
    "        positives = positives.to(device)\n",
    "        negatives = negatives.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        anchor_emb = model(anchors)\n",
    "        positive_emb = model(positives)\n",
    "        negative_emb = model(negatives)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, accuracy, dist_pos, dist_neg = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy\n",
    "        total_dist_pos += dist_pos\n",
    "        total_dist_neg += dist_neg\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "    avg_dist_pos = total_dist_pos / num_batches\n",
    "    avg_dist_neg = total_dist_neg / num_batches\n",
    "    \n",
    "    return avg_loss, avg_accuracy, avg_dist_pos, avg_dist_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abedb3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler],\n",
    "    device: torch.device,\n",
    "    num_epochs: int,\n",
    "    save_path: Path,\n",
    "    writer: Optional[SummaryWriter] = None,\n",
    "    early_stopping_patience: int = 10,\n",
    "    checkpoint_name:str|None = None,\n",
    "    scheduler_based_on_epoch: bool = False\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Full training loop with validation, early stopping, and TensorBoard logging.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_accuracy': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'lr': []\n",
    "    }\n",
    "\n",
    "    best_val_accuracy = -0.01\n",
    "    patience_counter = 0\n",
    "    global_step = 0\n",
    "\n",
    "    # Log model graph to TensorBoard\n",
    "    if writer is not None:\n",
    "        try:\n",
    "            dummy_input = torch.randn(1, int(SAMPLE_RATE * MAX_AUDIO_DURATION)).to(device)\n",
    "            writer.add_graph(model, dummy_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not log model graph: {e}\")\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc, train_dist_pos, train_dist_neg, global_step = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch,\n",
    "            writer=writer, global_step=global_step\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc, val_dist_pos, val_dist_neg = evaluate(\n",
    "            model, val_loader, criterion, device, desc=\"Validation\"\n",
    "        )\n",
    "\n",
    "        # Update learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(epoch if scheduler_based_on_epoch else val_loss) # type: ignore\n",
    "\n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_accuracy'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "\n",
    "        # Log epoch metrics to TensorBoard\n",
    "        if writer is not None:\n",
    "            # Loss\n",
    "            writer.add_scalar(\n",
    "                \"Train/Loss\",\n",
    "                train_loss,\n",
    "                epoch,\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"Validation/Loss\",\n",
    "                val_loss,\n",
    "                epoch,\n",
    "            )\n",
    "\n",
    "            # Accuracy\n",
    "            writer.add_scalar(\"Train/Accuracy\", train_acc, epoch)\n",
    "            writer.add_scalar(\"Validation/Accuracy\", val_acc, epoch)\n",
    "\n",
    "            # Distances\n",
    "            writer.add_scalar(\n",
    "                \"Train/Distance/Positive\",\n",
    "                train_dist_pos,\n",
    "                epoch,\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"Validation/Distance/Positive\",\n",
    "                val_dist_pos,\n",
    "                epoch,\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"Train/Distance/Negative\",\n",
    "                train_dist_neg,\n",
    "                epoch,\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"Validation/Distance/Negative\",\n",
    "                val_dist_neg,\n",
    "                epoch,\n",
    "            )\n",
    "\n",
    "            # Distance margin (negative - positive, should be > 0)\n",
    "            writer.add_scalar(\n",
    "                \"Train/Distance/Margin\", train_dist_neg - train_dist_pos, epoch\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"Validation/Distance/Margin\",\n",
    "                val_dist_neg - val_dist_pos,\n",
    "                epoch,\n",
    "            )\n",
    "\n",
    "            # Learning rate\n",
    "            writer.add_scalar('LearningRate', current_lr, epoch)\n",
    "\n",
    "        print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"val_accuracy\": val_acc,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"config\": {\n",
    "                        \"embedding_dim\": model.embedding_dim,\n",
    "                        \"hidden_dim\": model.hidden_dim,\n",
    "                        \"num_layers\": model.num_layers,\n",
    "                        \"sample_rate\": model.sample_rate,\n",
    "                        \"frame_size\": model.frame_size,\n",
    "                    },\n",
    "                },\n",
    "                save_path / (checkpoint_name or \"best_model.pt\"),\n",
    "            )\n",
    "            print(f\"✓ New best model saved! (Val Acc: {val_acc:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience: {patience_counter}/{early_stopping_patience}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch} epochs\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    checkpoint = torch.load(save_path / (checkpoint_name or 'best_model.pt'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"\\nLoaded best model from epoch {checkpoint['epoch']} with Val Acc: {checkpoint['val_accuracy']:.4f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ea9b9",
   "metadata": {},
   "source": [
    "## Train the model on synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f5a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard run: audio_embedding_20251210_184552\n",
      "Run 'tensorboard --logdir /home/anthony/Sync/mecomp-nextgen-analysis/Notebooks/../models/runs' to view logs\n",
      "\n",
      "============================================================\n",
      "Epoch 1/50\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b9db55e3e34f719b6d648810424f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b113ea2a21c54013a043b78cb4285fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1887 | Train Acc: 0.5506\n",
      "Val Loss: 0.1764 | Val Acc: 0.5960\n",
      "Learning Rate: 0.001000\n",
      "✓ New best model saved! (Val Acc: 0.5960)\n",
      "\n",
      "============================================================\n",
      "Epoch 2/50\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977ed84ca908459fa9aed87c7ddf0681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 [Train]:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0269fdcf7943a0918098108a5de39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1808 | Train Acc: 0.5857\n",
      "Val Loss: 0.1755 | Val Acc: 0.5922\n",
      "Learning Rate: 0.001000\n",
      "No improvement. Patience: 1/10\n",
      "\n",
      "============================================================\n",
      "Epoch 3/50\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc95f1b541a4805807988ff36a65b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 [Train]:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ac39ea63a540c788c6ad59a71a6d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1785 | Train Acc: 0.5867\n",
      "Val Loss: 0.1726 | Val Acc: 0.6023\n",
      "Learning Rate: 0.001000\n",
      "✓ New best model saved! (Val Acc: 0.6023)\n",
      "\n",
      "============================================================\n",
      "Epoch 4/50\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f7031ff4f8414f8ff6f2fe33d03fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 [Train]:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556120bdec1a40f4878487114dabc97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1806 | Train Acc: 0.5975\n",
      "Val Loss: 0.1672 | Val Acc: 0.6111\n",
      "Learning Rate: 0.001000\n",
      "✓ New best model saved! (Val Acc: 0.6111)\n",
      "\n",
      "============================================================\n",
      "Epoch 5/50\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410a3500b47d4014b0779e1adef7a72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 [Train]:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589e4e65fdad48c5af54178cc3996ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1669 | Train Acc: 0.6241\n",
      "Val Loss: 0.1672 | Val Acc: 0.6187\n",
      "Learning Rate: 0.001000\n",
      "✓ New best model saved! (Val Acc: 0.6187)\n",
      "\n",
      "============================================================\n",
      "Epoch 6/50\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d4651188db4c3ba96e07ad3b1d134c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 [Train]:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7896d3d35894403db2444514d753d8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1652 | Train Acc: 0.6266\n",
      "Val Loss: 0.1625 | Val Acc: 0.6301\n",
      "Learning Rate: 0.001000\n",
      "✓ New best model saved! (Val Acc: 0.6301)\n",
      "\n",
      "============================================================\n",
      "Epoch 7/50\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9dd5ea96f64fbeab3921a9ccc09bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 [Train]:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be088c342914424b0c582d5deeacaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1622 | Train Acc: 0.6309\n",
      "Val Loss: 0.1589 | Val Acc: 0.6465\n",
      "Learning Rate: 0.001000\n",
      "✓ New best model saved! (Val Acc: 0.6465)\n",
      "\n",
      "============================================================\n",
      "Epoch 8/50\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31f53e484df461aa3d228c3a73e29f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 [Train]:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9614e5f1cd64bf5b7155532f5776b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1619 | Train Acc: 0.6265\n",
      "Val Loss: 0.1581 | Val Acc: 0.6263\n",
      "Learning Rate: 0.001000\n",
      "No improvement. Patience: 1/10\n",
      "\n",
      "============================================================\n",
      "Epoch 9/50\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e892dff9184a30b0d152486ad617e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 [Train]:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671f70abecce48b485b3c5019ee63700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1581 | Train Acc: 0.6459\n",
      "Val Loss: 0.1542 | Val Acc: 0.6553\n",
      "Learning Rate: 0.001000\n",
      "✓ New best model saved! (Val Acc: 0.6553)\n",
      "\n",
      "============================================================\n",
      "Epoch 10/50\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6a14a24d874562948b42081e332eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 [Train]:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup training\n",
    "criterion = TripletMarginLossWithAccuracy(margin=0.2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "# Create TensorBoard writer with timestamp\n",
    "run_name = f\"audio_embedding_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_LOG_DIR / run_name)\n",
    "\n",
    "# Log hyperparameters\n",
    "hparams = {\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'frame_size': FRAME_SIZE,\n",
    "    'max_audio_duration': MAX_AUDIO_DURATION,\n",
    "    'sample_rate': SAMPLE_RATE,\n",
    "}\n",
    "writer.add_hparams(hparams, {})\n",
    "\n",
    "print(f\"TensorBoard run: {run_name}\")\n",
    "print(f\"Run 'tensorboard --logdir {TENSORBOARD_LOG_DIR.absolute()}' to view logs\")\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler, # type: ignore\n",
    "    device=DEVICE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    save_path=MODEL_SAVE_PATH,\n",
    "    writer=writer,\n",
    "    early_stopping_patience=10\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Test Evaluation\")\n",
    "print(\"=\"*60)\n",
    "test_loss, test_acc, test_dist_pos, test_dist_neg = evaluate(model, test_loader, criterion, DEVICE, desc=\"Test\")\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Log final test metrics\n",
    "writer.add_scalar('Test/Loss', test_loss, 0)\n",
    "writer.add_scalar('Test/Accuracy', test_acc, 0)\n",
    "writer.add_scalar('Test/DistancePositive', test_dist_pos, 0)\n",
    "writer.add_scalar('Test/DistanceNegative', test_dist_neg, 0)\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()\n",
    "print(\"\\nTensorBoard writer closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf7c9e4",
   "metadata": {},
   "source": [
    "## Finetune the model on human triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bebf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded best model from epoch 38 with Val Acc: 0.6913\n",
      "GPU memory allocated: 293.1 MB\n",
      "GPU memory reserved: 366.0 MB\n",
      "GPU memory cleared!\n"
     ]
    }
   ],
   "source": [
    "# Load the best model from previous training\n",
    "checkpoint = torch.load(MODEL_SAVE_PATH / \"best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"\\nLoaded best model from epoch {checkpoint['epoch']} with Val Acc: {checkpoint['val_accuracy']:.4f}\")\n",
    "\n",
    "# Clear GPU memory before fine-tuning\n",
    "import gc\n",
    "\n",
    "# Delete the previous dataloaders and datasets to free memory\n",
    "try:\n",
    "    del train_loader, val_loader, test_loader\n",
    "    del train_dataset, val_dataset, test_dataset\n",
    "except NameError:\n",
    "    ...\n",
    "\n",
    "# Clear Python garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2:.1f} MB\")\n",
    "\n",
    "print(\"GPU memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb2c18f",
   "metadata": {},
   "source": [
    "### Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64186b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a88026aba1470cb09bbb2985173d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating audio files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid triplets: 25 (removed 0 invalid)\n",
      "\n",
      "Creating validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251b57192ba2499d8ee38b90cb0adcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating audio files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid triplets: 13 (removed 0 invalid)\n",
      "\n",
      "Creating test dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92b8c0975d143bb8da775c5205d7cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating audio files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid triplets: 12 (removed 0 invalid)\n",
      "\n",
      "Train batches: 7\n",
      "Val batches: 4\n",
      "Test batches: 3\n"
     ]
    }
   ],
   "source": [
    "# Create datasets with full duration\n",
    "print(\"Creating training dataset...\")\n",
    "ft_train_dataset = TripletAudioDataset(\n",
    "    ft_train_triplets, FMA_SMALL_DIR, max_duration=FT_MAX_AUDIO_DURATION, cache_audio=True,\n",
    ")\n",
    "\n",
    "print(\"\\nCreating validation dataset...\")\n",
    "ft_val_dataset = TripletAudioDataset(\n",
    "    ft_val_triplets, FMA_SMALL_DIR, max_duration=FT_MAX_AUDIO_DURATION, cache_audio=True,\n",
    ")\n",
    "\n",
    "print(\"\\nCreating test dataset...\")\n",
    "ft_test_dataset = TripletAudioDataset(\n",
    "    ft_test_triplets, FMA_SMALL_DIR, max_duration=FT_MAX_AUDIO_DURATION, cache_audio=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Create dataloaders (num_workers=0 to avoid multiprocessing issues in notebooks)\n",
    "ft_train_loader = DataLoader(\n",
    "    ft_train_dataset,\n",
    "    batch_size=FT_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_triplets,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "ft_val_loader = DataLoader(\n",
    "    ft_val_dataset,\n",
    "    batch_size=FT_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_triplets,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "ft_test_loader = DataLoader(\n",
    "    ft_test_dataset,\n",
    "    batch_size=FT_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_triplets,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(ft_train_loader)}\")\n",
    "print(f\"Val batches: {len(ft_val_loader)}\")\n",
    "print(f\"Test batches: {len(ft_test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b92ec09",
   "metadata": {},
   "source": [
    "# Finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221609c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run_time is not defined, define it for fine-tuning\n",
    "try:\n",
    "    run_name\n",
    "except NameError:\n",
    "    run_name = f\"audio_embedding_{datetime.now().strftime('%Y%m%d_%H%M%S')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003daa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard run: finetuned_audio_embedding_20251210_180039\n",
      "Run 'tensorboard --logdir /home/anthony/Sync/mecomp-nextgen-analysis/Notebooks/../models/runs' to view logs\n",
      "\n",
      "============================================================\n",
      "Epoch 1/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4ab86261714eabba38a76bfbee2e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c63793dd8c417eac490e93429d8f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.2478 | Train Acc: 0.5357\n",
      "Val Loss: 0.2072 | Val Acc: 0.6250\n",
      "Learning Rate: 0.000010\n",
      "✓ New best model saved! (Val Acc: 0.6250)\n",
      "\n",
      "============================================================\n",
      "Epoch 2/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923f167d98754e51ae8c04a7938db0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efabf8e9c5f4706bd6cc4edf496aa9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1954 | Train Acc: 0.4643\n",
      "Val Loss: 0.2009 | Val Acc: 0.6250\n",
      "Learning Rate: 0.000010\n",
      "No improvement. Patience: 1/10\n",
      "\n",
      "============================================================\n",
      "Epoch 3/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420971ddced54aa3978f679c7db5f73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb794969b674ba7a735b014b0a7865d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.2213 | Train Acc: 0.4643\n",
      "Val Loss: 0.2602 | Val Acc: 0.5625\n",
      "Learning Rate: 0.000010\n",
      "No improvement. Patience: 2/10\n",
      "\n",
      "============================================================\n",
      "Epoch 4/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b7a55f08d7426ca11144a485bcdad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3c35493f094af688c89f664231bf21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.2172 | Train Acc: 0.5357\n",
      "Val Loss: 0.1908 | Val Acc: 0.6250\n",
      "Learning Rate: 0.000010\n",
      "No improvement. Patience: 3/10\n",
      "\n",
      "============================================================\n",
      "Epoch 5/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f60e2835e746dc9e246c00d66e2c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70dbe24c2b2f43998611d7f5ee2878a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1492 | Train Acc: 0.7143\n",
      "Val Loss: 0.1783 | Val Acc: 0.6250\n",
      "Learning Rate: 0.000010\n",
      "No improvement. Patience: 4/10\n",
      "\n",
      "============================================================\n",
      "Epoch 6/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3097d3545064fe4a6ae966999bc2d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d109521b70b4f3f9fa1670e6c0d6131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1646 | Train Acc: 0.6429\n",
      "Val Loss: 0.1985 | Val Acc: 0.6875\n",
      "Learning Rate: 0.000010\n",
      "✓ New best model saved! (Val Acc: 0.6875)\n",
      "\n",
      "============================================================\n",
      "Epoch 7/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910b3e484cdb4f2798d0c6750c7c942b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26271192bff445fe9620bdb7f2052344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1511 | Train Acc: 0.6429\n",
      "Val Loss: 0.2415 | Val Acc: 0.6250\n",
      "Learning Rate: 0.000010\n",
      "No improvement. Patience: 1/10\n",
      "\n",
      "============================================================\n",
      "Epoch 8/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd2bef9c1044390984bf558917cddaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3865e1b8985428eaecce1f070a020ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.2417 | Train Acc: 0.5357\n",
      "Val Loss: 0.1967 | Val Acc: 0.6250\n",
      "Learning Rate: 0.000010\n",
      "No improvement. Patience: 2/10\n",
      "\n",
      "============================================================\n",
      "Epoch 9/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d9fb06975e479984c8bf43f66aa935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbec49a31b44053b399ec86e39c130c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1642 | Train Acc: 0.5714\n",
      "Val Loss: 0.1998 | Val Acc: 0.5625\n",
      "Learning Rate: 0.000010\n",
      "No improvement. Patience: 3/10\n",
      "\n",
      "============================================================\n",
      "Epoch 10/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70e068f9d2645538b03025261e321eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735b2514a850409c85681a256d3548ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1641 | Train Acc: 0.6786\n",
      "Val Loss: 0.2274 | Val Acc: 0.6875\n",
      "Learning Rate: 0.000010\n",
      "No improvement. Patience: 4/10\n",
      "\n",
      "============================================================\n",
      "Epoch 11/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e83fafcb2fd4821bf4b031e6a379e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9103467a7ef94c348a6dfed240472e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1497 | Train Acc: 0.6429\n",
      "Val Loss: 0.2084 | Val Acc: 0.6250\n",
      "Learning Rate: 0.000010\n",
      "No improvement. Patience: 5/10\n",
      "\n",
      "============================================================\n",
      "Epoch 12/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a957d9387ad41d6a6493c2f5f481f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe63f7372904d4e9a1b8b3028c50c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1604 | Train Acc: 0.6071\n",
      "Val Loss: 0.1923 | Val Acc: 0.6250\n",
      "Learning Rate: 0.000005\n",
      "No improvement. Patience: 6/10\n",
      "\n",
      "============================================================\n",
      "Epoch 13/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b1a4f8e96c411d88e3c8b72e83132c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d84a0b2ffd4e15b8a1b0ce64af56e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.2104 | Train Acc: 0.5000\n",
      "Val Loss: 0.2150 | Val Acc: 0.5625\n",
      "Learning Rate: 0.000005\n",
      "No improvement. Patience: 7/10\n",
      "\n",
      "============================================================\n",
      "Epoch 14/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c72b03914942eea9bfa17de575b43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be86ad2f17674a2987cbbdfb3bdafa15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.2012 | Train Acc: 0.6071\n",
      "Val Loss: 0.1887 | Val Acc: 0.6875\n",
      "Learning Rate: 0.000005\n",
      "No improvement. Patience: 8/10\n",
      "\n",
      "============================================================\n",
      "Epoch 15/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3a85f4abd74bd1aa07f05b66e2fd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1885d5519e354e1180f48307c876ac6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1572 | Train Acc: 0.7143\n",
      "Val Loss: 0.2040 | Val Acc: 0.6250\n",
      "Learning Rate: 0.000005\n",
      "No improvement. Patience: 9/10\n",
      "\n",
      "============================================================\n",
      "Epoch 16/20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3302bdccae84a0fa80930734c6c8957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16 [Train]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690d13699037409da4a66c5338e16258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.1277 | Train Acc: 0.7500\n",
      "Val Loss: 0.1964 | Val Acc: 0.6250\n",
      "Learning Rate: 0.000005\n",
      "No improvement. Patience: 10/10\n",
      "\n",
      "Early stopping triggered after 16 epochs\n",
      "\n",
      "Loaded best model from epoch 6 with Val Acc: 0.6875\n",
      "\n",
      "============================================================\n",
      "Final Test Evaluation\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68afaa2aa9094278941a24c977256a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3481 | Test Accuracy: 0.1667\n",
      "\n",
      "TensorBoard writer closed.\n"
     ]
    }
   ],
   "source": [
    "# Setup training\n",
    "criterion = TripletMarginLossWithAccuracy(margin=0.2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=FT_LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "# Create TensorBoard writer with timestamp\n",
    "ft_run_name = f\"finetuned_{run_name}\" # append \"finetuned\" to the previous run\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_LOG_DIR / ft_run_name)\n",
    "\n",
    "# Log hyperparameters\n",
    "hparams = {\n",
    "    \"embedding_dim\": EMBEDDING_DIM,\n",
    "    \"hidden_dim\": HIDDEN_DIM,\n",
    "    \"num_layers\": NUM_LAYERS,\n",
    "    \"batch_size\": FT_BATCH_SIZE,\n",
    "    \"learning_rate\": FT_LEARNING_RATE,\n",
    "    \"frame_size\": FRAME_SIZE,\n",
    "    \"max_audio_duration\": FT_MAX_AUDIO_DURATION,\n",
    "    \"sample_rate\": SAMPLE_RATE,\n",
    "}\n",
    "writer.add_hparams(hparams, {})\n",
    "\n",
    "print(f\"TensorBoard run: {ft_run_name}\")\n",
    "print(f\"Run 'tensorboard --logdir {TENSORBOARD_LOG_DIR.absolute()}' to view logs\")\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=ft_train_loader,\n",
    "    val_loader=ft_val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,  # type: ignore\n",
    "    device=DEVICE,\n",
    "    num_epochs=FT_NUM_EPOCHS,\n",
    "    save_path=MODEL_SAVE_PATH,\n",
    "    writer=writer,\n",
    "    early_stopping_patience=10,\n",
    "    checkpoint_name=\"best_finetuned_model.pt\",\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Final Test Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "test_loss, test_acc, test_dist_pos, test_dist_neg = evaluate(\n",
    "    model, ft_test_loader, criterion, DEVICE, desc=\"Test\"\n",
    ")\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Log final test metrics\n",
    "writer.add_scalar(\"Test/Loss\", test_loss, 0)\n",
    "writer.add_scalar(\"Test/Accuracy\", test_acc, 0)\n",
    "writer.add_scalar(\"Test/DistancePositive\", test_dist_pos, 0)\n",
    "writer.add_scalar(\"Test/DistanceNegative\", test_dist_neg, 0)\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()\n",
    "print(\"\\nTensorBoard writer closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9606690a",
   "metadata": {},
   "source": [
    "## Export the model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5a714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to ONNX: ../models/audio_embedding_model.onnx\n",
      "File size: 3.09 MB\n",
      "ONNX model is valid!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1319213/376361822.py:30: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "/usr/lib/python3.13/site-packages/torch/onnx/_internal/torchscript_exporter/symbolic_opset9.py:4247: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with GRU can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def export_to_onnx(\n",
    "    model: nn.Module,\n",
    "    save_path: Path,\n",
    "    sample_rate: int = SAMPLE_RATE,\n",
    "    duration_seconds: float = 30.0,\n",
    "    opset_version: int = 14\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Export the model to ONNX format.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        save_path: Directory to save the ONNX file\n",
    "        sample_rate: Sample rate for the dummy input\n",
    "        duration_seconds: Duration of audio in seconds for the dummy input\n",
    "        opset_version: ONNX opset version\n",
    "        \n",
    "    Returns:\n",
    "        Path to the saved ONNX file\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Create dummy input (batch_size=1, num_samples)\n",
    "    num_samples = int(sample_rate * duration_seconds)\n",
    "    dummy_input = torch.randn(1, num_samples).to(next(model.parameters()).device)\n",
    "\n",
    "    onnx_path = save_path / \"audio_embedding_model.onnx\"\n",
    "\n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,  # type: ignore\n",
    "        onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=opset_version,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"audio\"],\n",
    "        output_names=[\"embedding\"],\n",
    "        dynamo=False,\n",
    "        dynamic_axes={\n",
    "            \"audio\": {0: \"batch_size\", 1: \"num_samples\"},\n",
    "            \"embedding\": {0: \"batch_size\"},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Model exported to ONNX: {onnx_path}\")\n",
    "    print(f\"File size: {onnx_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "    return onnx_path\n",
    "\n",
    "# Export the model\n",
    "onnx_path = export_to_onnx(model, MODEL_SAVE_PATH)\n",
    "\n",
    "# Verify the ONNX model\n",
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(str(onnx_path))\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model is valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab533c",
   "metadata": {},
   "source": [
    "## Smoke-test: try loading the model and running inferrence on a song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe959dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: ../data/fma_small/050/050444.mp3\n",
      "\n",
      "Loaded audio: 661560 samples (30.00 seconds)\n",
      "Inference time: 42.35 ms\n",
      "Embedding shape: (1, 32)\n",
      "Embedding (first 10 values): [ 0.34702694  0.04073701  0.10750052  0.24285202  0.04910754 -0.1841668\n",
      "  0.00205559 -0.07194922  0.09648682 -0.07768904]\n",
      "\n",
      "--- PyTorch comparison ---\n",
      "Loaded audio: 661560 samples (30.00 seconds)\n",
      "Inference time: 42.35 ms\n",
      "Embedding shape: (1, 32)\n",
      "Embedding (first 10 values): [ 0.34702694  0.04073701  0.10750052  0.24285202  0.04910754 -0.1841668\n",
      "  0.00205559 -0.07194922  0.09648682 -0.07768904]\n",
      "\n",
      "--- PyTorch comparison ---\n",
      "PyTorch embedding (first 10 values): [ 0.3470279   0.04074111  0.10750135  0.2428513   0.04910833 -0.18416363\n",
      "  0.00205329 -0.07194897  0.09648717 -0.07768948]\n",
      "\n",
      "Max difference between ONNX and PyTorch: 0.000004\n",
      "✓ ONNX export verified - outputs match!\n",
      "PyTorch embedding (first 10 values): [ 0.3470279   0.04074111  0.10750135  0.2428513   0.04910833 -0.18416363\n",
      "  0.00205329 -0.07194897  0.09648717 -0.07768948]\n",
      "\n",
      "Max difference between ONNX and PyTorch: 0.000004\n",
      "✓ ONNX export verified - outputs match!\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "def test_onnx_inference(onnx_path: Path, audio_path: Path, sample_rate: int = SAMPLE_RATE) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load ONNX model and run inference on an audio file.\n",
    "    \n",
    "    Args:\n",
    "        onnx_path: Path to the ONNX model\n",
    "        audio_path: Path to the audio file\n",
    "        sample_rate: Target sample rate\n",
    "        \n",
    "    Returns:\n",
    "        Embedding numpy array\n",
    "    \"\"\"\n",
    "    # Load audio using ffmpeg\n",
    "    audio = load_audio_ffmpeg(audio_path, sample_rate)\n",
    "    if audio is None:\n",
    "        raise ValueError(f\"Failed to load audio from {audio_path}\")\n",
    "    \n",
    "    print(f\"Loaded audio: {len(audio)} samples ({len(audio)/sample_rate:.2f} seconds)\")\n",
    "    \n",
    "    # Create ONNX runtime session\n",
    "    session = ort.InferenceSession(str(onnx_path), providers=['CPUExecutionProvider'])\n",
    "    \n",
    "    # Prepare input (add batch dimension)\n",
    "    audio_input = audio.reshape(1, -1).astype(np.float32)\n",
    "    \n",
    "    # Run inference\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    embedding: np.ndarray = session.run([output_name], {input_name: audio_input})[0]  # type: ignore\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Inference time: {inference_time*1000:.2f} ms\")\n",
    "    print(f\"Embedding shape: {embedding.shape}\")\n",
    "    print(f\"Embedding (first 10 values): {embedding[0, :10]}\")\n",
    "    \n",
    "    return embedding[0]\n",
    "\n",
    "\n",
    "# Find a sample audio file to test with\n",
    "sample_files = list(FMA_SMALL_DIR.glob(\"*/*.mp3\"))\n",
    "if sample_files:\n",
    "    test_audio_path = sample_files[0]\n",
    "    print(f\"Testing with: {test_audio_path}\\n\")\n",
    "    \n",
    "    # Test ONNX inference\n",
    "    embedding = test_onnx_inference(onnx_path, test_audio_path)\n",
    "    \n",
    "    # Also test PyTorch inference for comparison\n",
    "    print(\"\\n--- PyTorch comparison ---\")\n",
    "    audio = load_audio_ffmpeg(test_audio_path, SAMPLE_RATE)\n",
    "    audio_tensor = torch.from_numpy(audio).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pytorch_embedding = model(audio_tensor).cpu().numpy()[0]\n",
    "    \n",
    "    print(f\"PyTorch embedding (first 10 values): {pytorch_embedding[:10]}\")\n",
    "    \n",
    "    # Check if they're close\n",
    "    max_diff = np.abs(embedding - pytorch_embedding).max()\n",
    "    print(f\"\\nMax difference between ONNX and PyTorch: {max_diff:.6f}\")\n",
    "    \n",
    "    if max_diff < 1e-4:\n",
    "        print(\"✓ ONNX export verified - outputs match!\")\n",
    "    else:\n",
    "        print(\"⚠ Warning: ONNX and PyTorch outputs differ significantly\")\n",
    "else:\n",
    "    print(\"No audio files found for testing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
