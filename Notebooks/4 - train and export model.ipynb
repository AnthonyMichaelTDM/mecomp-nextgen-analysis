{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e689e09",
   "metadata": {},
   "source": [
    "# Train and export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e33151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Batch size: 16\n",
      "Max audio duration: 10.0s\n",
      "Frame size: 2048\n",
      "TensorBoard logs: /home/anthony/Sync/mecomp-nextgen-analysis/Notebooks/../models/runs\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuration\n",
    "SAMPLE_RATE = 22050  # Hz\n",
    "EMBEDDING_DIM = 64  # Output embedding size\n",
    "HIDDEN_DIM = 256     # GRU hidden dimension\n",
    "NUM_LAYERS = 1       # Number of GRU layers\n",
    "BATCH_SIZE = 16       # Small batch size to fit in GPU memory\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 5\n",
    "MAX_AUDIO_DURATION = 10.0  # Max duration in seconds\n",
    "FRAME_SIZE = 2048    # Frame size for CNN frontend\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "FMA_SMALL_DIR = DATA_DIR / \"fma_small\"\n",
    "HUMAN_TRIPLETS_PATH = DATA_DIR / \"human_triplets.csv\"\n",
    "SYNTHETIC_TRIPLETS_PATH = DATA_DIR / \"synthetic_triplets.csv\"\n",
    "MODEL_SAVE_PATH = Path(\"../models\")\n",
    "MODEL_SAVE_PATH.mkdir(exist_ok=True)\n",
    "TENSORBOARD_LOG_DIR = Path(\"../models/runs\")\n",
    "TENSORBOARD_LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Max audio duration: {MAX_AUDIO_DURATION}s\")\n",
    "print(f\"Frame size: {FRAME_SIZE}\")\n",
    "print(f\"TensorBoard logs: {TENSORBOARD_LOG_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f2c2c",
   "metadata": {},
   "source": [
    "## Load and process data\n",
    "\n",
    "Load the human and synthetic triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecac1d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human triplets after filtering: 20\n",
      "Human triplets: 20\n",
      "Synthetic triplets: 8000\n",
      "\n",
      "Human triplets sample:\n",
      "   anchor  positive  negative\n",
      "0  107389     25033    142433\n",
      "1  141164    123976    116883\n",
      "2   73675      5940     53586\n",
      "3  108808     12189     38450\n",
      "4  150078     25797    123834\n",
      "\n",
      "Synthetic triplets sample:\n",
      "   anchor  positive  negative\n",
      "0  145742     91306    108867\n",
      "1  123979     89846     90590\n",
      "2  123969     75930    152570\n",
      "3  123442    137900     71276\n",
      "4  141562     13767     59727\n"
     ]
    }
   ],
   "source": [
    "# Load triplet data\n",
    "human_triplets = pd.read_csv(HUMAN_TRIPLETS_PATH)\n",
    "synthetic_triplets = pd.read_csv(SYNTHETIC_TRIPLETS_PATH, skiprows=1)  # Skip the header row with column types\n",
    "\n",
    "# Filter human triplets to only keep rows where anchor and positive are set\n",
    "human_triplets = human_triplets.dropna(subset=['anchor', 'positive'])\n",
    "print(f\"Human triplets after filtering: {len(human_triplets)}\")\n",
    "\n",
    "# Set field types\n",
    "COLUMNS = [\n",
    "    \"anchor\",\n",
    "    \"positive\"\n",
    "]\n",
    "for column in COLUMNS:\n",
    "    human_triplets[column] = human_triplets[column].astype(int)\n",
    "\n",
    "# The synthetic triplets have duplicate column names, let's clean them up\n",
    "# Keep only the first 3 columns (anchor, positive, negative track_ids)\n",
    "synthetic_triplets = synthetic_triplets.iloc[:, :3]\n",
    "synthetic_triplets.columns = ['anchor', 'positive', 'negative']\n",
    "\n",
    "# Human triplets format: song1, song2, song3, anchor, positive\n",
    "# We need to convert to anchor, positive, negative format\n",
    "def convert_human_triplet(row):\n",
    "    \"\"\"Convert human triplet format to anchor/positive/negative format.\"\"\"\n",
    "    songs = [row['song1'], row['song2'], row['song3']]\n",
    "    anchor_idx = row['anchor'] - 1  # Convert 1-indexed to 0-indexed\n",
    "    positive_idx = row['positive'] - 1\n",
    "    negative_idx = 3 - anchor_idx - positive_idx  # The remaining index\n",
    "    return pd.Series({\n",
    "        'anchor': songs[anchor_idx],\n",
    "        'positive': songs[positive_idx],\n",
    "        'negative': songs[negative_idx]\n",
    "    }, dtype=int)\n",
    "\n",
    "human_triplets_converted = human_triplets.apply(convert_human_triplet, axis=1)\n",
    "\n",
    "print(f\"Human triplets: {len(human_triplets_converted)}\")\n",
    "print(f\"Synthetic triplets: {len(synthetic_triplets)}\")\n",
    "print(f\"\\nHuman triplets sample:\")\n",
    "print(human_triplets_converted.head())\n",
    "print(f\"\\nSynthetic triplets sample:\")\n",
    "print(synthetic_triplets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef65fbe",
   "metadata": {},
   "source": [
    "### split human triplets into testing and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a15a185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training triplets: 6400 (synthetic only)\n",
      "Validation triplets: 20 (human only)\n",
      "Test triplets: 1600 (validation only)\n"
     ]
    }
   ],
   "source": [
    "# Split the human and synthetic triplets into train/validation/test sets\n",
    "# For human:\n",
    "# - 0% train\n",
    "# - 0% test\n",
    "# - 100% validation\n",
    "# for synthetic:\n",
    "# - 80% train\n",
    "# - 20% test\n",
    "# - 0% validation\n",
    "\n",
    "\n",
    "# human_test, val_triplets = train_test_split(\n",
    "#     human_triplets_converted, test_size=0.9, random_state=42\n",
    "# )\n",
    "val_triplets = human_triplets_converted\n",
    "train_triplets, test_triplets = train_test_split(\n",
    "    synthetic_triplets, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Combine synthetic triplets with human data for ... dataset\n",
    "# test_triplets = pd.concat([human_test, synthetic_test], ignore_index=True)\n",
    "\n",
    "# (human: {len(human_test)}, synthetic: {len(synthetic_test)})\n",
    "print(f\"Training triplets: {len(train_triplets)} (synthetic only)\")\n",
    "print(f\"Validation triplets: {len(val_triplets)} (human only)\")\n",
    "print(\n",
    "    f\"Test triplets: {len(test_triplets)} (validation only)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7233c7",
   "metadata": {},
   "source": [
    "### create a dataloader with the raw audio for the songs for each triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd313e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d88c973597471b8b8ff482f75379b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating audio files:   0%|          | 0/6400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid triplets: 6400 (removed 0 invalid)\n",
      "\n",
      "Creating validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90603437a8e24f90bcb31abcfbb2a732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating audio files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid triplets: 20 (removed 0 invalid)\n",
      "\n",
      "Creating test dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c626f4d6a54aee8b871010c1739d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating audio files:   0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid triplets: 1600 (removed 0 invalid)\n",
      "\n",
      "Train batches: 400\n",
      "Val batches: 2\n",
      "Test batches: 100\n"
     ]
    }
   ],
   "source": [
    "def get_audio_path(track_id: int) -> Path:\n",
    "    \"\"\"Get the path to an audio file given its track ID.\"\"\"\n",
    "    # Track IDs are stored in folders based on first 3 digits\n",
    "    # e.g., track 123456 would be in fma_small/123/123456.mp3\n",
    "    tid_str = f\"{track_id:06d}\"\n",
    "    folder = tid_str[:3]\n",
    "    return FMA_SMALL_DIR / folder / f\"{tid_str}.mp3\"\n",
    "\n",
    "\n",
    "def load_audio_ffmpeg(path: Path, sample_rate: int = SAMPLE_RATE, max_duration: Optional[float] = None) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load audio file using ffmpeg and resample to target sample rate.\n",
    "    Returns mono audio as numpy array of float32 samples normalized to [-1, 1].\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    \n",
    "    cmd = [\n",
    "        'ffmpeg',\n",
    "        '-i', str(path),\n",
    "        '-f', 'f32le',        # 32-bit float little-endian PCM\n",
    "        '-acodec', 'pcm_f32le',\n",
    "        '-ac', '1',           # Mono\n",
    "        '-ar', str(sample_rate),  # Resample\n",
    "        '-v', 'quiet',        # Suppress output\n",
    "        '-'                   # Output to stdout\n",
    "    ]\n",
    "    \n",
    "    if max_duration is not None:\n",
    "        cmd.insert(1, '-t')\n",
    "        cmd.insert(2, str(max_duration))\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, check=True)\n",
    "        # Use .copy() to make the array writable (frombuffer returns read-only view)\n",
    "        audio = np.frombuffer(result.stdout, dtype=np.float32).copy()\n",
    "        return audio\n",
    "    except subprocess.CalledProcessError:\n",
    "        return None\n",
    "\n",
    "\n",
    "class TripletAudioDataset(Dataset):\n",
    "    \"\"\"Dataset that loads triplets of audio files.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        triplets_df: pd.DataFrame, \n",
    "        fma_dir: Path,\n",
    "        sample_rate: int = SAMPLE_RATE,\n",
    "        max_duration: float = 30.0,  # Max duration in seconds\n",
    "        cache_audio: bool = False\n",
    "    ):\n",
    "        self.triplets = triplets_df.reset_index(drop=True)\n",
    "        self.fma_dir = fma_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_duration = max_duration\n",
    "        self.max_samples = int(max_duration * sample_rate)\n",
    "        self.cache_audio = cache_audio\n",
    "        self._audio_cache = {}\n",
    "        \n",
    "        # Filter out triplets where any audio file is missing\n",
    "        valid_indices = []\n",
    "        for idx in tqdm(range(len(self.triplets)), desc=\"Validating audio files\"):\n",
    "            row = self.triplets.iloc[idx]\n",
    "            anchor_path = get_audio_path(int(row['anchor']))\n",
    "            positive_path = get_audio_path(int(row['positive']))\n",
    "            negative_path = get_audio_path(int(row['negative']))\n",
    "            \n",
    "            if anchor_path.exists() and positive_path.exists() and negative_path.exists():\n",
    "                valid_indices.append(idx)\n",
    "        \n",
    "        self.triplets = self.triplets.iloc[valid_indices].reset_index(drop=True)\n",
    "        print(f\"Valid triplets: {len(self.triplets)} (removed {len(triplets_df) - len(self.triplets)} invalid)\")\n",
    "    \n",
    "    def _load_audio(self, track_id: int) -> torch.Tensor:\n",
    "        \"\"\"Load and preprocess audio for a track.\"\"\"\n",
    "        if self.cache_audio and track_id in self._audio_cache:\n",
    "            return self._audio_cache[track_id]\n",
    "        \n",
    "        path = get_audio_path(track_id)\n",
    "        audio = load_audio_ffmpeg(path, self.sample_rate, self.max_duration)\n",
    "        \n",
    "        if audio is None:\n",
    "            # Return silence if loading fails\n",
    "            audio = np.zeros(self.max_samples, dtype=np.float32)\n",
    "        \n",
    "        # Pad or truncate to max_samples\n",
    "        if len(audio) < self.max_samples:\n",
    "            audio = np.pad(audio, (0, self.max_samples - len(audio)))\n",
    "        else:\n",
    "            audio = audio[:self.max_samples]\n",
    "        \n",
    "        audio_tensor = torch.from_numpy(audio).float()\n",
    "        \n",
    "        if self.cache_audio:\n",
    "            self._audio_cache[track_id] = audio_tensor\n",
    "        \n",
    "        return audio_tensor\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.triplets)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        row = self.triplets.iloc[idx]\n",
    "        \n",
    "        anchor = self._load_audio(int(row['anchor']))\n",
    "        positive = self._load_audio(int(row['positive']))\n",
    "        negative = self._load_audio(int(row['negative']))\n",
    "        \n",
    "        return anchor, positive, negative\n",
    "\n",
    "\n",
    "def collate_triplets(batch: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]):\n",
    "    \"\"\"Collate function for triplet batches.\"\"\"\n",
    "    anchors = torch.stack([item[0] for item in batch])\n",
    "    positives = torch.stack([item[1] for item in batch])\n",
    "    negatives = torch.stack([item[2] for item in batch])\n",
    "    return anchors, positives, negatives\n",
    "\n",
    "\n",
    "# Create datasets with reduced duration\n",
    "print(\"Creating training dataset...\")\n",
    "train_dataset = TripletAudioDataset(train_triplets, FMA_SMALL_DIR, max_duration=MAX_AUDIO_DURATION)\n",
    "\n",
    "print(\"\\nCreating validation dataset...\")\n",
    "val_dataset = TripletAudioDataset(val_triplets, FMA_SMALL_DIR, max_duration=MAX_AUDIO_DURATION)\n",
    "\n",
    "print(\"\\nCreating test dataset...\")\n",
    "test_dataset = TripletAudioDataset(test_triplets, FMA_SMALL_DIR, max_duration=MAX_AUDIO_DURATION)\n",
    "\n",
    "# Create dataloaders (num_workers=0 to avoid multiprocessing issues in notebooks)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_triplets,\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_triplets,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_triplets,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e21df",
   "metadata": {},
   "source": [
    "## Define the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da69210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioEmbeddingGRU(\n",
      "  (cnn_frontend): Sequential(\n",
      "    (0): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv1d(64, 128, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): AdaptiveAvgPool1d(output_size=1)\n",
      "  )\n",
      "  (gru): GRU(128, 256, batch_first=True)\n",
      "  (projection): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 420,672\n",
      "Trainable parameters: 420,672\n"
     ]
    }
   ],
   "source": [
    "class AudioEmbeddingGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU-based model for generating audio embeddings from raw audio frames.\n",
    "    \n",
    "    The model processes raw audio samples through:\n",
    "    1. A 1D CNN frontend to extract local features from the raw waveform\n",
    "    2. A GRU to capture temporal dependencies across the audio\n",
    "    3. A projection layer to produce the final embedding\n",
    "    \n",
    "    Args:\n",
    "        embedding_dim: Dimension of the output embedding (default: 128)\n",
    "        hidden_dim: Dimension of GRU hidden state (default: 256)\n",
    "        num_layers: Number of GRU layers (default: 2)\n",
    "        sample_rate: Expected sample rate of input audio (default: 22050)\n",
    "        frame_size: Size of each frame for the CNN frontend (default: 1024)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int = 128,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2,\n",
    "        sample_rate: int = 22050,\n",
    "        frame_size: int = 1024,\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.sample_rate = sample_rate\n",
    "        self.frame_size = frame_size\n",
    "        \n",
    "        # CNN frontend to extract features from raw audio frames\n",
    "        # Input: (batch, 1, frame_size) -> Output: (batch, cnn_out_dim)\n",
    "        self.cnn_frontend = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # # Third conv block\n",
    "            # nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.BatchNorm1d(256),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Global average pooling\n",
    "        )\n",
    "        \n",
    "        # Calculate CNN output dimension\n",
    "        self.cnn_out_dim = 128\n",
    "        \n",
    "        # GRU for temporal modeling\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.cnn_out_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Projection head to embedding space\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # L2 normalization for embeddings\n",
    "        self.normalize = True\n",
    "    \n",
    "    def _extract_frames(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract non-overlapping frames from raw audio.\n",
    "        Each frame is independent - no overlap between frames.\n",
    "        \n",
    "        Args:\n",
    "            audio: Raw audio tensor of shape (batch, num_samples)\n",
    "            \n",
    "        Returns:\n",
    "            Frames tensor of shape (batch, num_frames, frame_size)\n",
    "        \"\"\"\n",
    "        batch_size, num_samples = audio.shape\n",
    "        \n",
    "        # Calculate number of complete frames (non-overlapping)\n",
    "        num_frames = num_samples // self.frame_size\n",
    "        \n",
    "        # Truncate to complete frames only\n",
    "        usable_samples = num_frames * self.frame_size\n",
    "        audio_truncated = audio[:, :usable_samples]\n",
    "        \n",
    "        # Reshape to (batch, num_frames, frame_size)\n",
    "        frames = audio_truncated.view(batch_size, num_frames, self.frame_size)\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    def forward(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate embedding from raw audio.\n",
    "        \n",
    "        Args:\n",
    "            audio: Raw audio tensor of shape (batch, num_samples)\n",
    "            \n",
    "        Returns:\n",
    "            Embedding tensor of shape (batch, embedding_dim)\n",
    "        \"\"\"\n",
    "        batch_size = audio.shape[0]\n",
    "        \n",
    "        # Extract frames from audio (non-overlapping)\n",
    "        frames = self._extract_frames(audio)  # (batch, num_frames, frame_size)\n",
    "        num_frames = frames.shape[1]\n",
    "        \n",
    "        # Reshape for CNN: (batch * num_frames, 1, frame_size)\n",
    "        frames_flat = frames.reshape(-1, 1, self.frame_size)\n",
    "        \n",
    "        # Apply CNN frontend\n",
    "        cnn_out = self.cnn_frontend(frames_flat)  # (batch * num_frames, cnn_out_dim, 1)\n",
    "        cnn_out = cnn_out.squeeze(-1)  # (batch * num_frames, cnn_out_dim)\n",
    "        \n",
    "        # Reshape back to sequence: (batch, num_frames, cnn_out_dim)\n",
    "        cnn_out = cnn_out.view(batch_size, num_frames, -1)\n",
    "        \n",
    "        # Apply GRU\n",
    "        gru_out, hidden = self.gru(cnn_out)  # gru_out: (batch, num_frames, hidden_dim)\n",
    "        \n",
    "        # Use final hidden state for embedding\n",
    "        final_hidden = hidden[-1]  # (batch, hidden_dim)\n",
    "        \n",
    "        # Project to embedding space\n",
    "        embedding = self.projection(final_hidden)  # (batch, embedding_dim)\n",
    "        \n",
    "        # L2 normalize embeddings\n",
    "        if self.normalize:\n",
    "            embedding = F.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def get_embedding_dim(self) -> int:\n",
    "        \"\"\"Return the embedding dimension.\"\"\"\n",
    "        return self.embedding_dim\n",
    "\n",
    "\n",
    "# Create model instance with configured frame size\n",
    "model = AudioEmbeddingGRU(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    frame_size=FRAME_SIZE\n",
    ").to(DEVICE)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc300bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With 10.0s audio @ 22050Hz:\n",
      "  Samples per audio: 220,500\n",
      "  Frames per audio: 107\n",
      "  Frames per batch (×3 for triplet): 5,136\n",
      "\n",
      "Test input shape: torch.Size([2, 220500])\n",
      "Test output shape: torch.Size([2, 64])\n",
      "\n",
      "Test input shape: torch.Size([2, 220500])\n",
      "Test output shape: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "# Calculate memory usage estimate\n",
    "num_samples = int(SAMPLE_RATE * MAX_AUDIO_DURATION)\n",
    "num_frames = num_samples // FRAME_SIZE\n",
    "print(f\"\\nWith {MAX_AUDIO_DURATION}s audio @ {SAMPLE_RATE}Hz:\")\n",
    "print(f\"  Samples per audio: {num_samples:,}\")\n",
    "print(f\"  Frames per audio: {num_frames}\")\n",
    "print(f\"  Frames per batch (×3 for triplet): {BATCH_SIZE * 3 * num_frames:,}\")\n",
    "\n",
    "# Quick test with dummy input\n",
    "dummy_input = torch.randn(2, int(SAMPLE_RATE * MAX_AUDIO_DURATION)).to(DEVICE)\n",
    "dummy_output = model(dummy_input)\n",
    "print(f\"\\nTest input shape: {dummy_input.shape}\")\n",
    "print(f\"Test output shape: {dummy_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08c248",
   "metadata": {},
   "source": [
    "## Train and Test loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee93b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletMarginLossWithAccuracy(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet margin loss with accuracy tracking.\n",
    "    \n",
    "    For triplet (anchor, positive, negative), the loss is:\n",
    "    max(0, margin + d(anchor, positive) - d(anchor, negative))\n",
    "    \n",
    "    Accuracy is measured as the percentage of triplets where:\n",
    "    d(anchor, positive) < d(anchor, negative)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, margin: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        anchor: torch.Tensor, \n",
    "        positive: torch.Tensor, \n",
    "        negative: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, float, float, float]:\n",
    "        \"\"\"\n",
    "        Compute triplet loss and accuracy.\n",
    "        \n",
    "        Args:\n",
    "            anchor: Anchor embeddings (batch, embedding_dim)\n",
    "            positive: Positive embeddings (batch, embedding_dim)\n",
    "            negative: Negative embeddings (batch, embedding_dim)\n",
    "            \n",
    "        Returns:\n",
    "            loss: Scalar loss tensor\n",
    "            accuracy: Float accuracy (0-1)\n",
    "            avg_dist_pos: Average distance to positive\n",
    "            avg_dist_neg: Average distance to negative\n",
    "        \"\"\"\n",
    "        # Compute L2 distances\n",
    "        dist_pos = F.pairwise_distance(anchor, positive, p=2)\n",
    "        dist_neg = F.pairwise_distance(anchor, negative, p=2)\n",
    "        \n",
    "        # Triplet margin loss\n",
    "        losses = F.relu(self.margin + dist_pos - dist_neg)\n",
    "        loss = losses.mean()\n",
    "        \n",
    "        # Accuracy: positive should be closer than negative\n",
    "        correct = (dist_pos < dist_neg).float()\n",
    "        accuracy = correct.mean().item()\n",
    "        \n",
    "        # Average distances for logging\n",
    "        avg_dist_pos = dist_pos.mean().item()\n",
    "        avg_dist_neg = dist_neg.mean().item()\n",
    "        \n",
    "        return loss, accuracy, avg_dist_pos, avg_dist_neg\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    writer: Optional[SummaryWriter] = None,\n",
    "    global_step: int = 0\n",
    ") -> Tuple[float, float, float, float, int]:\n",
    "    \"\"\"Train for one epoch with TensorBoard logging.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_dist_pos = 0.0\n",
    "    total_dist_neg = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch} [Train]\")\n",
    "    for anchors, positives, negatives in pbar:\n",
    "        anchors = anchors.to(device)\n",
    "        positives = positives.to(device)\n",
    "        negatives = negatives.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        anchor_emb = model(anchors)\n",
    "        positive_emb = model(positives)\n",
    "        negative_emb = model(negatives)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, accuracy, dist_pos, dist_neg = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy\n",
    "        total_dist_pos += dist_pos\n",
    "        total_dist_neg += dist_neg\n",
    "        num_batches += 1\n",
    "        global_step += 1\n",
    "        \n",
    "        # Log batch metrics to TensorBoard\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('Train/BatchLoss', loss.item(), global_step)\n",
    "            writer.add_scalar('Train/BatchAccuracy', accuracy, global_step)\n",
    "            writer.add_scalar('Train/GradNorm', grad_norm.item(), global_step)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{accuracy:.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "    avg_dist_pos = total_dist_pos / num_batches\n",
    "    avg_dist_neg = total_dist_neg / num_batches\n",
    "    \n",
    "    return avg_loss, avg_accuracy, avg_dist_pos, avg_dist_neg, global_step\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    desc: str = \"Eval\"\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"Evaluate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_dist_pos = 0.0\n",
    "    total_dist_neg = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for anchors, positives, negatives in tqdm(dataloader, desc=desc):\n",
    "        anchors = anchors.to(device)\n",
    "        positives = positives.to(device)\n",
    "        negatives = negatives.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        anchor_emb = model(anchors)\n",
    "        positive_emb = model(positives)\n",
    "        negative_emb = model(negatives)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, accuracy, dist_pos, dist_neg = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy\n",
    "        total_dist_pos += dist_pos\n",
    "        total_dist_neg += dist_neg\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "    avg_dist_pos = total_dist_pos / num_batches\n",
    "    avg_dist_neg = total_dist_neg / num_batches\n",
    "    \n",
    "    return avg_loss, avg_accuracy, avg_dist_pos, avg_dist_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abedb3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler],\n",
    "    device: torch.device,\n",
    "    num_epochs: int,\n",
    "    save_path: Path,\n",
    "    writer: Optional[SummaryWriter] = None,\n",
    "    early_stopping_patience: int = 10\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Full training loop with validation, early stopping, and TensorBoard logging.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_accuracy': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'lr': []\n",
    "    }\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "    global_step = 0\n",
    "\n",
    "    # Log model graph to TensorBoard\n",
    "    if writer is not None:\n",
    "        try:\n",
    "            dummy_input = torch.randn(1, int(SAMPLE_RATE * MAX_AUDIO_DURATION)).to(device)\n",
    "            writer.add_graph(model, dummy_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not log model graph: {e}\")\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc, train_dist_pos, train_dist_neg, global_step = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch,\n",
    "            writer=writer, global_step=global_step\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc, val_dist_pos, val_dist_neg = evaluate(\n",
    "            model, val_loader, criterion, device, desc=\"Validation\"\n",
    "        )\n",
    "\n",
    "        # Update learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(epoch)\n",
    "\n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_accuracy'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "\n",
    "        # Log epoch metrics to TensorBoard\n",
    "        if writer is not None:\n",
    "            # Loss\n",
    "            writer.add_scalars(\n",
    "                \"Train/Loss\",\n",
    "                train_loss,\n",
    "                {\"Train\": train_loss, \"Validation\": val_loss},\n",
    "                epoch,\n",
    "            )\n",
    "            writer.add_scalars(\n",
    "                \"Validation/Loss\",\n",
    "                val_loss,\n",
    "                epoch,\n",
    "            )\n",
    "\n",
    "            # Accuracy\n",
    "            writer.add_scalars(\"Train/Accuracy\", train_acc, epoch)\n",
    "            writer.add_scalars(\"Validation/Accuracy\", val_acc, epoch)\n",
    "\n",
    "            # Distances\n",
    "            writer.add_scalars(\n",
    "                \"Train/Distance/Positive\",\n",
    "                train_dist_pos,\n",
    "                epoch,\n",
    "            )\n",
    "            writer.add_scalars(\n",
    "                \"Validation/Distance/Positive\",\n",
    "                val_dist_pos,\n",
    "                epoch,\n",
    "            )\n",
    "            writer.add_scalars(\n",
    "                \"Train/Distance/Negative\",\n",
    "                train_dist_neg,\n",
    "                epoch,\n",
    "            )\n",
    "            writer.add_scalars(\n",
    "                \"Validation/Distance/Negative\",\n",
    "                val_dist_neg,\n",
    "                epoch,\n",
    "            )\n",
    "\n",
    "            # Distance margin (negative - positive, should be > 0)\n",
    "            writer.add_scalars(\n",
    "                \"Train/Distance/Margin\", train_dist_neg - train_dist_pos, epoch\n",
    "            )\n",
    "            writer.add_scalars(\n",
    "                \"Validation/Distance/Margin\",\n",
    "                val_dist_neg - val_dist_pos,\n",
    "                epoch,\n",
    "            )\n",
    "\n",
    "            # Learning rate\n",
    "            writer.add_scalar('LearningRate', current_lr, epoch)\n",
    "\n",
    "        print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_accuracy': val_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'config': {\n",
    "                    'embedding_dim': model.embedding_dim,\n",
    "                    'hidden_dim': model.hidden_dim,\n",
    "                    'num_layers': model.num_layers,\n",
    "                    'sample_rate': model.sample_rate,\n",
    "                    'frame_size': model.frame_size,\n",
    "                }\n",
    "            }, save_path / 'best_model.pt')\n",
    "            print(f\"✓ New best model saved! (Val Acc: {val_acc:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience: {patience_counter}/{early_stopping_patience}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch} epochs\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    checkpoint = torch.load(save_path / 'best_model.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"\\nLoaded best model from epoch {checkpoint['epoch']} with Val Acc: {checkpoint['val_accuracy']:.4f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ea9b9",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f5a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard run: audio_embedding_20251205_025057\n",
      "Run 'tensorboard --logdir /home/anthony/Sync/mecomp-nextgen-analysis/Notebooks/../models/runs' to view logs\n",
      "\n",
      "============================================================\n",
      "Epoch 1/5\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39c79412aff4b4fbe5ae59a43cb6474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup training\n",
    "criterion = TripletMarginLossWithAccuracy(margin=0.2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "# Create TensorBoard writer with timestamp\n",
    "run_name = f\"audio_embedding_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_LOG_DIR / run_name)\n",
    "\n",
    "# Log hyperparameters\n",
    "hparams = {\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'frame_size': FRAME_SIZE,\n",
    "    'max_audio_duration': MAX_AUDIO_DURATION,\n",
    "    'sample_rate': SAMPLE_RATE,\n",
    "}\n",
    "writer.add_hparams(hparams, {})\n",
    "\n",
    "print(f\"TensorBoard run: {run_name}\")\n",
    "print(f\"Run 'tensorboard --logdir {TENSORBOARD_LOG_DIR.absolute()}' to view logs\")\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    save_path=MODEL_SAVE_PATH,\n",
    "    writer=writer,\n",
    "    early_stopping_patience=10\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Test Evaluation\")\n",
    "print(\"=\"*60)\n",
    "test_loss, test_acc, test_dist_pos, test_dist_neg = evaluate(model, test_loader, criterion, DEVICE, desc=\"Test\")\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Log final test metrics\n",
    "writer.add_scalar('Test/Loss', test_loss, 0)\n",
    "writer.add_scalar('Test/Accuracy', test_acc, 0)\n",
    "writer.add_scalar('Test/DistancePositive', test_dist_pos, 0)\n",
    "writer.add_scalar('Test/DistanceNegative', test_dist_neg, 0)\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()\n",
    "print(\"\\nTensorBoard writer closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9606690a",
   "metadata": {},
   "source": [
    "## Export the model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_onnx(\n",
    "    model: nn.Module,\n",
    "    save_path: Path,\n",
    "    sample_rate: int = SAMPLE_RATE,\n",
    "    duration_seconds: float = 30.0,\n",
    "    opset_version: int = 14\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Export the model to ONNX format.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        save_path: Directory to save the ONNX file\n",
    "        sample_rate: Sample rate for the dummy input\n",
    "        duration_seconds: Duration of audio in seconds for the dummy input\n",
    "        opset_version: ONNX opset version\n",
    "        \n",
    "    Returns:\n",
    "        Path to the saved ONNX file\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy input (batch_size=1, num_samples)\n",
    "    num_samples = int(sample_rate * duration_seconds)\n",
    "    dummy_input = torch.randn(1, num_samples).to(next(model.parameters()).device)\n",
    "    \n",
    "    onnx_path = save_path / \"audio_embedding_model.onnx\"\n",
    "    \n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=opset_version,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['audio'],\n",
    "        output_names=['embedding'],\n",
    "        dynamic_axes={\n",
    "            'audio': {0: 'batch_size', 1: 'num_samples'},\n",
    "            'embedding': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Model exported to ONNX: {onnx_path}\")\n",
    "    print(f\"File size: {onnx_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    return onnx_path\n",
    "\n",
    "# Export the model\n",
    "onnx_path = export_to_onnx(model, MODEL_SAVE_PATH)\n",
    "\n",
    "# Verify the ONNX model\n",
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(str(onnx_path))\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model is valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab533c",
   "metadata": {},
   "source": [
    "## Smoke-test: try loading the model and running inferrence on a song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe959dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "def test_onnx_inference(onnx_path: Path, audio_path: Path, sample_rate: int = SAMPLE_RATE) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load ONNX model and run inference on an audio file.\n",
    "    \n",
    "    Args:\n",
    "        onnx_path: Path to the ONNX model\n",
    "        audio_path: Path to the audio file\n",
    "        sample_rate: Target sample rate\n",
    "        \n",
    "    Returns:\n",
    "        Embedding numpy array\n",
    "    \"\"\"\n",
    "    # Load audio using ffmpeg\n",
    "    audio = load_audio_ffmpeg(audio_path, sample_rate)\n",
    "    if audio is None:\n",
    "        raise ValueError(f\"Failed to load audio from {audio_path}\")\n",
    "    \n",
    "    print(f\"Loaded audio: {len(audio)} samples ({len(audio)/sample_rate:.2f} seconds)\")\n",
    "    \n",
    "    # Create ONNX runtime session\n",
    "    session = ort.InferenceSession(str(onnx_path), providers=['CPUExecutionProvider'])\n",
    "    \n",
    "    # Prepare input (add batch dimension)\n",
    "    audio_input = audio.reshape(1, -1).astype(np.float32)\n",
    "    \n",
    "    # Run inference\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    embedding = session.run([output_name], {input_name: audio_input})[0]\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Inference time: {inference_time*1000:.2f} ms\")\n",
    "    print(f\"Embedding shape: {embedding.shape}\")\n",
    "    print(f\"Embedding (first 10 values): {embedding[0, :10]}\")\n",
    "    \n",
    "    return embedding[0]\n",
    "\n",
    "\n",
    "# Find a sample audio file to test with\n",
    "sample_files = list(FMA_SMALL_DIR.glob(\"*/*.mp3\"))\n",
    "if sample_files:\n",
    "    test_audio_path = sample_files[0]\n",
    "    print(f\"Testing with: {test_audio_path}\\n\")\n",
    "    \n",
    "    # Test ONNX inference\n",
    "    embedding = test_onnx_inference(onnx_path, test_audio_path)\n",
    "    \n",
    "    # Also test PyTorch inference for comparison\n",
    "    print(\"\\n--- PyTorch comparison ---\")\n",
    "    audio = load_audio_ffmpeg(test_audio_path, SAMPLE_RATE)\n",
    "    audio_tensor = torch.from_numpy(audio).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pytorch_embedding = model(audio_tensor).cpu().numpy()[0]\n",
    "    \n",
    "    print(f\"PyTorch embedding (first 10 values): {pytorch_embedding[:10]}\")\n",
    "    \n",
    "    # Check if they're close\n",
    "    max_diff = np.abs(embedding - pytorch_embedding).max()\n",
    "    print(f\"\\nMax difference between ONNX and PyTorch: {max_diff:.6f}\")\n",
    "    \n",
    "    if max_diff < 1e-4:\n",
    "        print(\"✓ ONNX export verified - outputs match!\")\n",
    "    else:\n",
    "        print(\"⚠ Warning: ONNX and PyTorch outputs differ significantly\")\n",
    "else:\n",
    "    print(\"No audio files found for testing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
